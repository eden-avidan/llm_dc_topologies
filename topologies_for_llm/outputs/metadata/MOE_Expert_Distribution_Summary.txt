================================================================================
MOE WORKLOAD EXPERT CONFIGURATION SUMMARY
================================================================================

GLOBAL PARAMETERS (same for all 123 MOE workloads):
  - Number of Experts:    16
  - Router Top-K:         2 (each token routed to 2 experts)
  - Grouped GEMM:         Enabled
  - GPU Type:             A100

================================================================================
EXPERT PARALLELISM (EP) BREAKDOWN
================================================================================

The 16 experts are distributed across EP ranks as follows:

┌────────┬─────────────────────┬──────────────┬──────────────────┐
│   EP   │ Experts per EP Rank │ Workloads    │ Distribution     │
├────────┼─────────────────────┼──────────────┼──────────────────┤
│   2    │         8           │      5       │ 16 ÷ 2  = 8     │
│   4    │         4           │     16       │ 16 ÷ 4  = 4     │
│   8    │         2           │     33       │ 16 ÷ 8  = 2     │
│   16   │         1           │     69       │ 16 ÷ 16 = 1     │
└────────┴─────────────────────┴──────────────┴──────────────────┘

Most common: EP=16 (69 workloads) with 1 expert per EP rank

================================================================================
HOW EXPERTS ARE DISTRIBUTED ACROSS GPUs
================================================================================

The total GPU count (world_size) is divided among 4 parallelism dimensions:

    world_size = TP × PP × EP × DP

Where:
  - TP (Tensor Parallelism):    Split model layers across GPUs
  - PP (Pipeline Parallelism):  Split model stages across GPUs  
  - EP (Expert Parallelism):    Split experts across GPUs
  - DP (Data Parallelism):      Replicate entire model

Expert Distribution Example 1:
  world_size=1024, tp=16, pp=2, ep=16
  → DP = 1024 / (16 × 2 × 16) = 2
  → 2 complete data-parallel replicas
  → Each replica has 16 experts distributed across 16 EP ranks
  → Each EP rank holds 1 expert, sharded across 16 TP GPUs
  → 2 pipeline stages per replica

Expert Distribution Example 2:
  world_size=512, tp=8, pp=8, ep=8
  → DP = 512 / (8 × 8 × 8) = 1
  → 1 model replica (no data parallelism)
  → 16 experts distributed across 8 EP ranks
  → Each EP rank holds 2 experts, sharded across 8 TP GPUs
  → 8 pipeline stages

Expert Distribution Example 3:
  world_size=1024, tp=4, pp=8, ep=4
  → DP = 1024 / (4 × 8 × 4) = 8
  → 8 complete data-parallel replicas
  → Each replica has 16 experts across 4 EP ranks
  → Each EP rank holds 4 experts, sharded across 4 TP GPUs
  → 8 pipeline stages per replica

================================================================================
COMMUNICATION PATTERNS FOR MOE
================================================================================

MOE layers introduce additional communication:

1. ALLTOALL_EP: Token routing between EP ranks
   - Tokens are routed to their assigned experts
   - Appears as "mlp_moelayer" operations in workload files

2. ALLGATHER/REDUCESCATTER: Within expert computation
   - Standard TP communication within each expert
   - Each expert is tensor-parallelized

3. ALLGATHER_DP_EP / REDUCESCATTER_DP_EP: For gradient norms
   - Combined DP and EP communication
   - Synchronization across data and expert parallel groups

================================================================================
KEY INSIGHTS
================================================================================

• Higher EP = More fine-grained expert distribution
  - EP=16: Maximum distribution (1 expert/rank)
  - EP=2:  Minimum distribution (8 experts/rank)

• EP affects communication overhead:
  - Higher EP → More ALLTOALL communication between experts
  - Lower EP → More computation per EP rank

• The non-MOE ("Densed") equivalent:
  - Has ep=1 (no expert parallelism)
  - Uses standard MLP layers instead of MOE layers
  - Same TP, PP, DP configuration otherwise

================================================================================
